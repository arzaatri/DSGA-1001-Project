{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of features to drop made it worse apparently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import surgery_preprocess # A .py with preprocessing code\n",
    "# The models we'll use, minus XGBoost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score,train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score, accuracy_score, f1_score, average_precision_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# HyperOpt is a hyperparameter tuning package\n",
    "from hyperopt import fmin, tpe, anneal, hp, Trials, space_eval\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "surgery=surgery_preprocess(split=False)\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True)\n",
    "ros = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols=['FVC', 'FEV1', 'Age', 'FEV1/FVC', 'FVC_deficit', 'FEV1_deficit',\n",
    "       'FEV1/FVC_deficit', 'FEV1^2', 'FVC^2', 'Age*FVC', 'Age*FEV1',\n",
    "       'FVC*FEV1', 'FVC^2*FEV1', 'FVC*FEV1^2']\n",
    "def split(surgery, drop_cols=[], idx=[]):\n",
    "    d = drop_cols.copy()\n",
    "    num_col = numeric_cols.copy()\n",
    "    print(d)\n",
    "    if len(d) > 0:\n",
    "        for el in d:\n",
    "            num_col.remove(el)\n",
    "    d.append('Risk1Yr')\n",
    "    if len(idx)==0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(surgery.drop(columns=d),\n",
    "                                                        surgery['Risk1Yr'], \n",
    "                                                        test_size=0.2,\n",
    "                                                        shuffle=True,\n",
    "                                                        stratify=surgery['Risk1Yr'],)\n",
    "    else:\n",
    "        X_train = surgery.loc[idx].drop(columns=d)\n",
    "        X_test = surgery.loc[~surgery.index.isin(idx)].drop(columns=d)\n",
    "        y_train = surgery.loc[idx, 'Risk1Yr']\n",
    "        y_test = surgery.loc[~surgery.index.isin(idx),'Risk1Yr']\n",
    "        \n",
    "    ### Normalize numeric features ###\n",
    "    scaler = StandardScaler()\n",
    "    # Fit the test data in the same scale as the training data\n",
    "    X_train[num_col] = scaler.fit_transform(X_train[num_col])\n",
    "    X_test[num_col] = scaler.transform(X_test[num_col])\n",
    "\n",
    "    ### Return the split train/test data ###\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "drop_1 = ['Pain','Weakness','Dyspnoea','Type2_diabetes',\n",
    "                                     'FVC^2','FEV1^2']\n",
    "drop_2 = ['Pain','Weakness','Dyspnoea','Type2_diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea for improvement: Make it so you can specify which scores to return\n",
    "# and return them in a dict\n",
    "\n",
    "def get_scores(pipe, train_X, test_X, train_y, test_y, scores, n=7):\n",
    "    model_scores = {}\n",
    "    for s in scores:\n",
    "        model_scores[s] = 0\n",
    "    pipe.fit(train_X, train_y)\n",
    "\n",
    "    for i in range(n):\n",
    "        preds = pipe.predict(test_X)\n",
    "        proba = pipe.predict_proba(test_X)[:,1]\n",
    "        for s in scores:\n",
    "            if s in ['roc_auc_score', 'average_precition_score']:\n",
    "                model_scores[s] += eval(s+'(test_y, proba)')/n\n",
    "            else:\n",
    "                model_scores[s] += eval(s+'(test_y, preds)')/n\n",
    "    \n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "def plot_scores(model_scores, scores=None):\n",
    "    if scores == None:\n",
    "        scores = list(model_scores.keys())\n",
    "    if len(scores) > 5:\n",
    "        nrow = len(scores)//2 - 1\n",
    "        ncol = len(scores)//2 + len(scores)%2\n",
    "    else:\n",
    "        nrow = len(scores)//2 + len(scores)%2\n",
    "        ncol = 2\n",
    "    print(nrow, ncol)\n",
    "    fig, ax = plt.subplots(nrow,ncol,figsize=(12,12))\n",
    "    matplotlib.rc('xtick', labelsize=13) \n",
    "    matplotlib.rc('ytick', labelsize=13) \n",
    "    num = model_scores[scores[0]].shape[0]\n",
    "    i = 0\n",
    "    for s in scores:\n",
    "        ax[i//ncol,i%ncol].bar(range(1,num+1), model_scores[s])\n",
    "        ax[i//ncol,i%ncol].hlines(model_scores[s].mean(),1,num)\n",
    "        ax[i//ncol,i%ncol].set_xlabel('Trial', size=14)\n",
    "        ax[i//ncol,i%ncol].set_ylabel(s, size=14)\n",
    "        #ax[i//2,i%2].set_title(names[i], size=14)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model should be the function to call, ie LogisticRegression,\n",
    "# NOT LogisticRegression() - note the lack of parenthesis. Similarly,\n",
    "# obj should be the objective function. Model args is a dict of hyperparameters\n",
    "# that you may want to set manually for the chosen model\n",
    "# return_idx if you want to return the train indexes for each split, allowing yo to\n",
    "# replicate that split\n",
    "def build_model(model, params, obj, scores, k=8, n=10, evals=70, scoring='roc_auc',\n",
    "                data=surgery, drop=[], return_idx=False, **model_args):\n",
    "    print(drop)\n",
    "    scores_dict={}\n",
    "    param_list=[]\n",
    "    if return_idx:\n",
    "        idx_list = []\n",
    "    for s in scores:\n",
    "        scores_dict[s] = np.zeros(k)\n",
    "        \n",
    "    for i in range(k):\n",
    "        X_train, X_test, y_train, y_test = split(data, drop_cols=drop)\n",
    "        mod = model(**model_args)\n",
    "        pipe = Pipeline([('upsample',ros),('model',mod)])\n",
    "        trials = Trials()\n",
    "        # Partial make its own callable function. When you call partial() you supply\n",
    "        # a function with n args and supply k of them. Then, when you call the function\n",
    "        # that partial creates, you supply the remaining n-k. These n-k serve as the\n",
    "        # FIRST arguments to the function of which you made a partial\n",
    "        fmin_func = partial(obj, pipe=pipe, train_X=X_train, train_y=y_train,\n",
    "                            scoring=scoring)\n",
    "        best = fmin(fn=fmin_func, \n",
    "                    space=params, algo=anneal.suggest, max_evals=evals,\n",
    "                    trials=trials)\n",
    "        best_params = space_eval(params,best)\n",
    "        \n",
    "        pipe.fit(X_train,y_train)\n",
    "        pipe.set_params(**best_params)\n",
    "        model_scores = get_scores(pipe, X_train, X_test, y_train, y_test,\n",
    "                                  scores, n=n)\n",
    "        for s in scores:\n",
    "            scores_dict[s][i] = model_scores[s]\n",
    "        param_list.append(best_params)\n",
    "        if return_idx:\n",
    "            idx_list.append(X_train.index)\n",
    "        \n",
    "    if return_idx:\n",
    "        return scores_dict, param_list, idx_list\n",
    "    return scores_dict, param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_objective(params, pipe, train_X, train_y, scoring='roc_auc'):\n",
    "    pipe.set_params(**params)\n",
    "    score = cross_val_score(pipe, train_X, train_y, cv=kf, scoring=scoring)\n",
    "    return 1-score.mean()\n",
    "\n",
    "scores = ['roc_auc_score','recall_score', 'accuracy_score',\n",
    "          'average_precision_score']\n",
    "\n",
    "lr_params = {'model__C': hp.uniform('model__C', 0,500)}\n",
    "\n",
    "lr_args = {'max_iter': 2000,'C': 1.0000001}\n",
    "lr_scores, lr_params, lr_idx = build_model(LogisticRegression, lr_params, \n",
    "                                   lr_objective, scores, k=4, n=6, evals=30,\n",
    "                                   return_idx=True, **lr_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say we want to redo a representative trial, we can redo the split like so:\n",
    "X_train, X_test, y_train, y_test = split(surgery, drop_cols=['FEV1^2'], idx=lr_idx[1])\n",
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_scores(lr_scores, ['recall_score', 'roc_auc_score', 'accuracy_score', 'average_precision_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing AUC varies A LOT based on the intial split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def lg_objective(params, pipe, train_X, train_y, scoring):\n",
    "    pipe.set_params(**params)\n",
    "    score = cross_val_score(pipe, train_X, train_y, cv=kf, scoring=scoring).mean()\n",
    "    return 1-score\n",
    "\n",
    "lg_params = {'model__n_estimators': hp.choice('model__n_estimators', range(20,80)),\n",
    "             'model__learning_rate': hp.uniform('model__learning_rate',0.001,0.35),\n",
    "             'model__max_depth': hp.choice('model__max_depth', range(3,9)),\n",
    "             'model__bagging_fraction ': hp.uniform('model__subsample', 0.7, 1.0),\n",
    "             'model__lambda_l2': hp.uniform('model__lambda_l2', 0.1,21.0),\n",
    "             'model__num_leaves': hp.choice('model__num_leaves',range(5,50)),\n",
    "             'model__feature_fraction': hp.uniform('model__feature_fraction',0.6,1.0)}\n",
    "\n",
    "cat_feats = list(range(2,11))+list(range(23,27))\n",
    "\n",
    "lg_args = {'metric': 'binary_logloss', 'categorical_features': cat_feats,\n",
    "           'early_stopping_rounds': 4}\n",
    "scores = ['roc_auc_score','recall_score', 'accuracy_score',\n",
    "          'average_precision_score']\n",
    "\n",
    "\n",
    "lg_scores, lg_params = build_model(lgb.LGBMClassifier, lg_params, lg_objective,\n",
    "                                   scores, k=6, n=8, scoring='roc_auc', **lg_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in lg_scores.keys():\n",
    "    print(f'Mean {k}: {lg_scores[k].mean()}')\n",
    "plot_scores(lg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "lg_pipe = Pipeline([('upsample',ros),('model',lgb.LGBMClassifier(**lg_args))])\n",
    "lg_pipe.set_params(**lg_params[np.argmax(lr_scores['roc_auc_score'])])\n",
    "X_train, X_test, y_train, y_test = split(surgery)\n",
    "lg_pipe.fit(X_train,y_train)\n",
    "lg_proba = lg_pipe.predict_proba(X_test)[:,1]\n",
    "dat = pd.DataFrame({'num':range(1,len(lg_proba)+1),\n",
    "                    'proba': lg_proba,\n",
    "                    'class': y_test})\n",
    "dat.sort_values('class', inplace=True)\n",
    "dat['num'] = range(1,len(lg_proba)+1)\n",
    "dat.sort_values('num', inplace=True)\n",
    "sns.catplot(x='num', y='proba', hue='class', kind='bar', data=dat)\n",
    "plt.hlines(dat.loc[dat['class']==0,'proba'].mean(),0,93,colors='blue')\n",
    "plt.hlines(dat.loc[dat['class']==1,'proba'].mean(),0,93,colors='orange')\n",
    "plt.hlines(0.5,0,93,colors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(params, pipe, train_X, train_y, scoring='roc_auc'):\n",
    "    pipe.set_params(**params)\n",
    "    score = cross_val_score(pipe, train_X, train_y, cv=kf, scoring=scoring).mean()\n",
    "    return 1-score\n",
    "\n",
    "rf_params = {'model__n_estimators': hp.choice('model__n_estimators', range(70,300)),\n",
    "             'model__min_samples_leaf': hp.choice('model__min_samples_leaf',range(1,21)),\n",
    "             'model__min_samples_split': hp.choice('model__min_samples_split',range(2,21)),\n",
    "             'model__max_features': hp.uniform('model__max_features', 0.5, 0.9),\n",
    "             'model__max_depth': hp.choice('model__max_depth', range(3,26))}\n",
    "\n",
    "scores = ['roc_auc_score','recall_score', 'accuracy_score',\n",
    "          'average_precision_score']\n",
    "\n",
    "\n",
    "rf_scores, rf_params = build_model(RandomForestClassifier, rf_params, rf_objective,\n",
    "                                   scores, k=6, n=8, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
