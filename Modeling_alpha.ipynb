{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to check:\n",
    "\n",
    "-Maybe we have too many features? Try dropping some of the less important\n",
    "categorical/engineered ones\n",
    "\n",
    "-Other methods of upsampling? Maybe mix up and down sampling?\n",
    "\n",
    "-Tuning parameters of SMOTE? Cursory look says 2 neighbors works best\n",
    "\n",
    "-Maybe the paper just used training AUC??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FVC', 'FEV1', 'Performance', 'Pain', 'Haemoptysis', 'Dyspnoea',\n",
       "       'Cough', 'Weakness', 'Tumor_size', 'Type2_diabetes', 'MI_6months',\n",
       "       'PAD', 'Smoking', 'Asthma', 'Age', 'FEV1/FVC', 'FVC_deficit',\n",
       "       'FEV1_deficit', 'FEV1/FVC_deficit', 'FEV1^2', 'FVC^2', 'Age*FVC',\n",
       "       'Age*FEV1', 'FVC*FEV1', 'FVC^2*FEV1', 'FVC*FEV1^2', '_DGN2', '_DGN3',\n",
       "       '_DGN4', '_DGN5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import surgery_preprocess # A .py with preprocessing code\n",
    "# The models we'll use, minus XGBoost\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# For eval\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, recall_score, accuracy_score\n",
    "# imblearn is 'imbalanced learn,' an sklearn-compatible package for\n",
    "# dealing with imbalanced data\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTENC, BorderlineSMOTE, SMOTE, ADASYN,KMeansSMOTE, SVMSMOTE\n",
    "from scipy.stats import uniform\n",
    "# HyperOpt is a hyperparameter tuning package\n",
    "from hyperopt import fmin, tpe, anneal, hp, Trials, space_eval\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = surgery_preprocess()\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a few things that will be used in several cells\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True)\n",
    "# SMOTENC is built to work with categorical data. ROS randomly resamples the\n",
    "# minority class. Similar performance so far, but ROS is faster\n",
    "smoter_nc = SMOTENC(categorical_features = list(range(2,14))+list(range(26,30)),\n",
    "                             k_neighbors=2)\n",
    "ros = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperOpt requires you to manually build the objective function which\n",
    "# it will try to minimize. Only argument needed is params, which are the\n",
    "# hyperparameters it tries. Might be able to use other parameters to the\n",
    "# objective using functools?\n",
    "def lr_objective(params):\n",
    "    lr_pipe.set_params(**params)\n",
    "    score = cross_val_score(lr_pipe, X, y, cv=cv, scoring='roc_auc')\n",
    "    return 1-score.mean()\n",
    "lr = LogisticRegression()\n",
    "lr_pipe = Pipeline([('upsample', smoter_nc), ('model', lr)])\n",
    "lr_params = {'model__C': hp.uniform('model__C', 0,1000)}\n",
    "lr_trials = Trials() # Stores logging information\n",
    "\n",
    "# This call is what actually optimizer parameters\n",
    "lr_best = fmin(fn = lr_objective, space=lr_params, \n",
    "               algo=anneal.suggest, max_evals = 200,\n",
    "               trials=lr_trials)\n",
    "lr_best_params = space_eval(lr_params, lr_best) # Retrieves those parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "lr_pipe.set_params(**lr_best_params)\n",
    "lr_pipe.fit(X_train, y_train) \n",
    "print(roc_auc_score(y_test, lr_pipe.predict_proba(X_test)[:,1]))\n",
    "print(recall_score(y_test, lr_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_objective(params):\n",
    "    sv_pipe.set_params(**params)\n",
    "    score = cross_val_score(sv_pipe, X_train, y_train, cv=kf, scoring='roc_auc')\n",
    "    return 1-score.mean()\n",
    "\n",
    "# I tried a gaussian kernel and it often failed to fit. Maybe\n",
    "# worth playing with sigmoid/poly, or tuning class_weights?\n",
    "sv = svm.SVC(kernel='linear', probability=True)\n",
    "sv_pipe = Pipeline([('upsample', ros), ('model', sv)])\n",
    "sv_params = {'model__C': hp.uniform('model__C', 0,200)}\n",
    "sv_trials = Trials()\n",
    "sv_best = fmin(fn=svm_objective, space=sv_params,\n",
    "           algo=anneal.suggest, max_evals=50,\n",
    "           trials=sv_trials)\n",
    "\n",
    "sv_best_params = space_eval(sv_params, sv_trials.argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "sv_pipe.set_params(**sv_best_params)\n",
    "sv_pipe.fit(X_train, y_train) \n",
    "print(roc_auc_score(y_test, sv_pipe.predict_proba(X_test)[:,1]))\n",
    "print(recall_score(y_test, sv_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(params):\n",
    "    rf_pipe.set_params(**params)\n",
    "    score = cross_val_score(rf_pipe, X_train_t, y_train, cv=kf, scoring='recall').mean()\n",
    "    return 1-score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_pipe = Pipeline([('upsample', ros), ('model', rf)])\n",
    "# These search spaces are kinda arbitrary\n",
    "rf_params = {'model__n_estimators': hp.choice('model__n_estimators', range(50,200)),\n",
    "             'model__min_samples_leaf': hp.choice('model__min_samples_leaf',range(1,21)),\n",
    "             'model__min_samples_split': hp.choice('model__min_samples_split',range(2,21)),\n",
    "            'model__max_features': hp.uniform('model__max_features', 0.5, 1.0),\n",
    "            'model__max_depth': hp.choice('model__max_depth', range(3,10))}\n",
    "rf_trials = Trials()\n",
    "rf_best = fmin(fn=rf_objective, space=rf_params, algo=tpe.suggest,\n",
    "              max_evals=100,trials=rf_trials)\n",
    "rf_best_params = space_eval(rf_params, rf_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "f_pipe.set_params(**rf_best_params)\n",
    "f_pipe.fit(X_train, y_train) \n",
    "print(roc_auc_score(y_test, rf_pipe.predict_proba(X_test)[:,1]))\n",
    "print(recall_score(y_test, rf_pipe.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
